version: '3.8'

services:
  higgs-audio-tts:
    build: .
    image: higgs-audio-tts:latest
    container_name: higgs-audio-tts
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=bosonai/higgs-audio-v2-generation-3B-base
      - AUDIO_TOKENIZER_PATH=bosonai/higgs-audio-v2-tokenizer
      - DEVICE=auto
      - LOG_LEVEL=INFO
      - AUDIO_CODEC=auto  # auto | torchcodec | ffmpeg
    volumes:
      # Optional: Mount local cache for models
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # CPU-only variant (uncomment to use)
  # higgs-audio-tts-cpu:
  #   build: .
  #   image: higgs-audio-tts:latest
  #   container_name: higgs-audio-tts-cpu
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - MODEL_PATH=bosonai/higgs-audio-v2-generation-3B-base
  #     - AUDIO_TOKENIZER_PATH=bosonai/higgs-audio-v2-tokenizer
  #     - DEVICE=cpu
  #     - LOG_LEVEL=INFO
  #     - AUDIO_CODEC=ffmpeg  # Use FFmpeg for CPU-only deployments
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   restart: unless-stopped
